{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_Normalization(X, norm_mode):\n",
    "    num_Patient, num_Feature = np.shape(X)\n",
    "\n",
    "    if norm_mode == 'standard': #zero mean unit variance\n",
    "        for j in range(num_Feature):\n",
    "            if np.std(X[:,j]) != 0:\n",
    "                X[:,j] = (X[:,j] - np.mean(X[:, j]))/np.std(X[:,j])\n",
    "            else:\n",
    "                X[:,j] = (X[:,j] - np.mean(X[:, j]))\n",
    "    elif norm_mode == 'normal': #min-max normalization\n",
    "        for j in range(num_Feature):\n",
    "            X[:,j] = (X[:,j] - np.min(X[:,j]))/(np.max(X[:,j]) - np.min(X[:,j]))\n",
    "    else:\n",
    "        print(\"INPUT MODE ERROR!\")\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./cleaned_features_final.csv')\n",
    "df2 = pd.read_csv('./label.csv')\n",
    "\n",
    "data = np.asarray(df1)\n",
    "data = f_get_Normalization(data, 'standard')\n",
    "\n",
    "time = np.asarray(df2[['event_time']])\n",
    "label = np.asarray(df2[['label']])\n",
    "\n",
    "num_Category = int(np.max(time)*1.2)\n",
    "num_Event = int(len(np.unique(label))-1) \n",
    "\n",
    "mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n",
    "for i in range(np.shape(time)[0]):\n",
    "    if label[i,0] != 0:  #not censored\n",
    "        mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
    "    else: #label[i,2]==0: censored\n",
    "        mask[i,:,int(time[i,0]+1):] =  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, df2, test_size=0.2)\n",
    "\n",
    "x_train = torch.from_numpy(np.asarray(x_train)).float()\n",
    "y_train = torch.from_numpy(np.asarray(y_train))\n",
    "\n",
    "x_test = torch.from_numpy(np.asarray(x_test)).float()\n",
    "y_test = torch.from_numpy(np.asarray(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "  def __init__(self, data, label, mask):\n",
    "    self.data = data\n",
    "    self.label = label\n",
    "    self.mask = mask\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \n",
    "    data = self.data[idx]\n",
    "    label = self.label[idx]\n",
    "    mask = self.mask[idx]\n",
    "    return data, label, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataSet(x_train, y_train, mask)\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = False)\n",
    "test_dataset = DataSet(x_test,y_test, mask)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shared_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_dim = 79\n",
    "        self.h_dim = 200\n",
    "        self.output_dim = 79 \n",
    "        self.network = nn.Sequential(\n",
    "                nn.Linear(self.input_dim,self.h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.h_dim,self.h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.h_dim,self.h_dim),\n",
    "                nn.ReLU(),    \n",
    "                nn.Linear(self.h_dim,self.output_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "       \n",
    "    def forward(self,x): \n",
    "        raw_x = x\n",
    "        x= self.network(x)\n",
    "        x = torch.concat([x, raw_x], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_network = shared_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_dim = 79*2\n",
    "        self.h_dim = 200\n",
    "        self.output_dim = num_Category\n",
    "        self.network = nn.Sequential(\n",
    "                nn.Linear(self.input_dim,self.h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.h_dim,self.h_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.h_dim,self.h_dim),\n",
    "                nn.ReLU(),    \n",
    "                nn.Linear(self.h_dim,self.output_dim),\n",
    "                nn.ReLU(),               \n",
    "                )\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.network(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deephit(nn.Module): \n",
    "    def __init__(self,shared_network):\n",
    "        super().__init__()\n",
    "        self.shared_network = shared_network\n",
    "        self.num_event = num_Event\n",
    "        self.num_category = num_Category\n",
    "        self.h_dim = 200\n",
    "        self.sub_networks = nn.ModuleList([sub_network() for _ in range(self.num_event)])\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(self.num_event * self.num_category, self.num_event * self.num_category),\n",
    "            nn.Softmax(dim=1),\n",
    "            )\n",
    "\n",
    "    def forward(self, x): \n",
    "        shared_output = self.shared_network(x)\n",
    "        sub_outputs = [sub_net(shared_output) for sub_net in self.sub_networks]\n",
    "        out = torch.stack(sub_outputs, axis=1)\n",
    "        out = torch.reshape(out, [-1, self.num_event * self.num_category])\n",
    "        out = self.nn(out)\n",
    "        out = torch.reshape(out, [-1, self.num_event, self.num_category])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_L1(output,label, mask):\n",
    "        I_1 = torch.sign(label[:,1])\n",
    "        \n",
    "        #for uncenosred: log P(T=t,K=k|x)\n",
    "        tmp1 = torch.sum(torch.sum(mask * output,dim=2), dim=1, keepdim=True)\n",
    "        tmp1 = I_1 * torch.log(tmp1)\n",
    "\n",
    "        #for censored: log \\sum P(T>t|x)\n",
    "        tmp2 = torch.sum(torch.sum(mask * output,dim=2), dim=1, keepdim=True)\n",
    "        tmp2 = (1. - I_1) * torch.log(tmp2)\n",
    "\n",
    "        return -1* torch.mean(tmp1 + 1.0*tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "deephit = Deephit(shared_network)\n",
    "optimizer = optim.Adam(deephit.parameters(), lr=1e-4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model ,train_loader, optimizer):\n",
    "  deephit.train()\n",
    "\n",
    "  train_loss = 0\n",
    "  \n",
    "  for data, label, mask in train_loader:\n",
    "    data = data\n",
    "    label = label\n",
    "    mask = mask\n",
    "    \n",
    "    output = deephit(data)\n",
    "    loss = loss_L1(output, label, mask)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "    \n",
    "  train_loss /= len(train_loader)\n",
    "  return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "  deephit.eval()\n",
    "  test_loss = 0\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for data, label, mask in train_loader:\n",
    "        data = data\n",
    "        label = label\n",
    "        mask = mask\n",
    "\n",
    "        output = deephit(data)\n",
    "        test_loss += loss_L1(output, label, mask).item()\n",
    "    \n",
    "  test_loss /= len(test_loader)\n",
    "  return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 0], \tTrain Loss: 4.4181, \tVal Loss: 15.9695\n",
      "[EPOCH: 1], \tTrain Loss: 3.8263, \tVal Loss: 13.0124\n",
      "[EPOCH: 2], \tTrain Loss: 3.3541, \tVal Loss: 11.8247\n",
      "[EPOCH: 3], \tTrain Loss: 2.9477, \tVal Loss: 9.3260\n",
      "[EPOCH: 4], \tTrain Loss: 2.2974, \tVal Loss: 6.4974\n",
      "[EPOCH: 5], \tTrain Loss: 1.6517, \tVal Loss: 4.3488\n",
      "[EPOCH: 6], \tTrain Loss: 1.2282, \tVal Loss: 3.2609\n",
      "[EPOCH: 7], \tTrain Loss: 0.9407, \tVal Loss: 2.5226\n",
      "[EPOCH: 8], \tTrain Loss: 0.7723, \tVal Loss: 2.0867\n",
      "[EPOCH: 9], \tTrain Loss: 0.6455, \tVal Loss: 1.7586\n",
      "[EPOCH: 10], \tTrain Loss: 0.5545, \tVal Loss: 1.5410\n",
      "[EPOCH: 11], \tTrain Loss: 0.4863, \tVal Loss: 1.3814\n",
      "[EPOCH: 12], \tTrain Loss: 0.4387, \tVal Loss: 1.2372\n",
      "[EPOCH: 13], \tTrain Loss: 0.4004, \tVal Loss: 1.1228\n",
      "[EPOCH: 14], \tTrain Loss: 0.3702, \tVal Loss: 1.0237\n",
      "[EPOCH: 15], \tTrain Loss: 0.3227, \tVal Loss: 0.9116\n",
      "[EPOCH: 16], \tTrain Loss: 0.2925, \tVal Loss: 0.8651\n",
      "[EPOCH: 17], \tTrain Loss: 0.2780, \tVal Loss: 0.7676\n",
      "[EPOCH: 18], \tTrain Loss: 0.2661, \tVal Loss: 0.7534\n",
      "[EPOCH: 19], \tTrain Loss: 0.2501, \tVal Loss: 0.7053\n",
      "[EPOCH: 20], \tTrain Loss: 0.2236, \tVal Loss: 0.6661\n",
      "[EPOCH: 21], \tTrain Loss: 0.2136, \tVal Loss: 0.7087\n",
      "[EPOCH: 22], \tTrain Loss: 0.2198, \tVal Loss: 0.6196\n",
      "[EPOCH: 23], \tTrain Loss: 0.2115, \tVal Loss: 0.5801\n",
      "[EPOCH: 24], \tTrain Loss: 0.1969, \tVal Loss: 0.5869\n",
      "[EPOCH: 25], \tTrain Loss: 0.1845, \tVal Loss: 0.5211\n",
      "[EPOCH: 26], \tTrain Loss: 0.1779, \tVal Loss: 0.5044\n",
      "[EPOCH: 27], \tTrain Loss: 0.1721, \tVal Loss: 0.5024\n",
      "[EPOCH: 28], \tTrain Loss: 0.1646, \tVal Loss: 0.5097\n",
      "[EPOCH: 29], \tTrain Loss: 0.1573, \tVal Loss: 0.4740\n",
      "[EPOCH: 30], \tTrain Loss: 0.1614, \tVal Loss: 0.4656\n",
      "[EPOCH: 31], \tTrain Loss: 0.1478, \tVal Loss: 0.4640\n",
      "[EPOCH: 32], \tTrain Loss: 0.1499, \tVal Loss: 0.4363\n",
      "[EPOCH: 33], \tTrain Loss: 0.1348, \tVal Loss: 0.4336\n",
      "[EPOCH: 34], \tTrain Loss: 0.1330, \tVal Loss: 0.4133\n",
      "[EPOCH: 35], \tTrain Loss: 0.1309, \tVal Loss: 0.4114\n",
      "[EPOCH: 36], \tTrain Loss: 0.1409, \tVal Loss: 0.3906\n",
      "[EPOCH: 37], \tTrain Loss: 0.1318, \tVal Loss: 0.3918\n",
      "[EPOCH: 38], \tTrain Loss: 0.1227, \tVal Loss: 0.3896\n",
      "[EPOCH: 39], \tTrain Loss: 0.1179, \tVal Loss: 0.3884\n",
      "[EPOCH: 40], \tTrain Loss: 0.1184, \tVal Loss: 0.3695\n",
      "[EPOCH: 41], \tTrain Loss: 0.1128, \tVal Loss: 0.3910\n",
      "[EPOCH: 42], \tTrain Loss: 0.1195, \tVal Loss: 0.3680\n",
      "[EPOCH: 43], \tTrain Loss: 0.1149, \tVal Loss: 0.3678\n",
      "[EPOCH: 44], \tTrain Loss: 0.1208, \tVal Loss: 0.3634\n",
      "[EPOCH: 45], \tTrain Loss: 0.1126, \tVal Loss: 0.3810\n",
      "[EPOCH: 46], \tTrain Loss: 0.1310, \tVal Loss: 0.4141\n",
      "[EPOCH: 47], \tTrain Loss: 0.1245, \tVal Loss: 0.4273\n",
      "[EPOCH: 48], \tTrain Loss: 0.1339, \tVal Loss: 0.3829\n",
      "[EPOCH: 49], \tTrain Loss: 0.1183, \tVal Loss: 0.3893\n",
      "[EPOCH: 50], \tTrain Loss: 0.1259, \tVal Loss: 0.3807\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "  train_loss = train(deephit, train_loader, optimizer)\n",
    "  val_loss = evaluate(deephit, test_loader)\n",
    "  print(f\"[EPOCH: {epoch}], \\tTrain Loss: {train_loss:.4f}, \\tVal Loss: {val_loss:.4f}\")\n",
    "  result = {\n",
    "    'EPOCH': epoch,\n",
    "    'Train Loss': train_loss,\n",
    "    'Val Loss': val_loss,\n",
    "    # 'Val Accuracy': val_accuracy\n",
    "    }\n",
    "  \n",
    "  result_list.append(result)\n",
    "result_df = pd.DataFrame(result_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tobigs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
