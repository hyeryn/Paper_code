{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from scipy.stats import norm\n",
    "from torchvision import datasets, transforms, utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_raw = datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "test_set_raw = datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "train_set = list(filter(lambda i: i[1] == 3 or i[1]==8 , train_set_raw))\n",
    "test_set = list(filter(lambda i: i[1] == 3 or i[1]==8 , test_set_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_set)): \n",
    "    if train_set[i][1]==3:\n",
    "        lst=list(train_set[i])\n",
    "        lst.append(0)\n",
    "        lst.pop(1)\n",
    "        train_set[i] = tuple(lst)\n",
    "\n",
    "    elif train_set[i][1]==8:\n",
    "        lst=list(train_set[i])\n",
    "        lst.append(1)\n",
    "        lst.pop(1)\n",
    "        train_set[i] = tuple(lst)\n",
    "    \n",
    "    else :\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[8][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=32,shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32,shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "EPOCHS = 200\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(nn.Module):\n",
    "    def __init__(self,input_dim=784):\n",
    "        super(FeatureSelector, self).__init__()\n",
    "        self.mu = torch.nn.Parameter(0.01*torch.randn(input_dim, ), requires_grad=True)\n",
    "        self.eps = torch.randn(self.mu.size()) #평균이 0이고 표준편차 1인 가우시안 정규분포에서 랜덤 노이즈(epsilon)\n",
    "        self.sigma = 1.0\n",
    "    \n",
    "    def hard_sigmoid(self, x):\n",
    "        return torch.clamp(x+0.5, 0.0, 1.0)\n",
    "\n",
    "    def forward(self, prev_x):\n",
    "        z = self.mu + self.sigma*self.eps.normal_()\n",
    "        stochastic_gate = self.hard_sigmoid(z)\n",
    "        new_x = prev_x * stochastic_gate\n",
    "        return new_x\n",
    "    \n",
    "    def regularizer(self): #규제 \n",
    "        x = ( self.mu + 0.5 ) / self.sigma\n",
    "        return 0.5 * (1 + torch.erf(x / math.sqrt(2))) \n",
    "\n",
    "\n",
    "    def get_gates(self):\n",
    "        return self.mu.detach().cpu().numpy(), np.minimum(1.0, np.maximum(0.0, self.mu.detach().cpu().numpy() + 0.5)) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector = FeatureSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "class original(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.NN =nn.Sequential(\n",
    "            nn.Linear(784,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,784)\n",
    "        x = self.NN(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.NN =nn.Sequential(\n",
    "            nn.Linear(784,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.feature_selector = feature_selector\n",
    "        self.regularizer = feature_selector.regularizer\n",
    "        self.get_gates = feature_selector.get_gates\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,784)\n",
    "        x = feature_selector(x)\n",
    "        x = self.NN(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NET()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "   \n",
    "    for data, target in train_loader:\n",
    "        input_x, target_x = data, target\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_x)\n",
    "        regularizer = torch.mean(model.regularizer())\n",
    "        loss = criterion(output,target_x) + 0.3 * regularizer \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH: 0], \tTrain Loss: 0.6723\n",
      "[EPOCH: 1], \tTrain Loss: 0.5696\n",
      "[EPOCH: 2], \tTrain Loss: 0.5615\n",
      "[EPOCH: 3], \tTrain Loss: 0.5550\n",
      "[EPOCH: 4], \tTrain Loss: 0.5521\n",
      "[EPOCH: 5], \tTrain Loss: 0.5473\n",
      "[EPOCH: 6], \tTrain Loss: 0.5416\n",
      "[EPOCH: 7], \tTrain Loss: 0.5371\n",
      "[EPOCH: 8], \tTrain Loss: 0.5319\n",
      "[EPOCH: 9], \tTrain Loss: 0.5301\n",
      "[EPOCH: 10], \tTrain Loss: 0.5267\n",
      "[EPOCH: 11], \tTrain Loss: 0.5219\n",
      "[EPOCH: 12], \tTrain Loss: 0.5164\n",
      "[EPOCH: 13], \tTrain Loss: 0.5143\n",
      "[EPOCH: 14], \tTrain Loss: 0.5112\n",
      "[EPOCH: 15], \tTrain Loss: 0.5078\n",
      "[EPOCH: 16], \tTrain Loss: 0.5027\n",
      "[EPOCH: 17], \tTrain Loss: 0.5012\n",
      "[EPOCH: 18], \tTrain Loss: 0.4962\n",
      "[EPOCH: 19], \tTrain Loss: 0.4952\n",
      "[EPOCH: 20], \tTrain Loss: 0.4907\n",
      "[EPOCH: 21], \tTrain Loss: 0.4887\n",
      "[EPOCH: 22], \tTrain Loss: 0.4864\n",
      "[EPOCH: 23], \tTrain Loss: 0.4846\n",
      "[EPOCH: 24], \tTrain Loss: 0.4811\n",
      "[EPOCH: 25], \tTrain Loss: 0.4781\n",
      "[EPOCH: 26], \tTrain Loss: 0.4768\n",
      "[EPOCH: 27], \tTrain Loss: 0.4746\n",
      "[EPOCH: 28], \tTrain Loss: 0.4721\n",
      "[EPOCH: 29], \tTrain Loss: 0.4691\n",
      "[EPOCH: 30], \tTrain Loss: 0.4672\n",
      "[EPOCH: 31], \tTrain Loss: 0.4649\n",
      "[EPOCH: 32], \tTrain Loss: 0.4624\n",
      "[EPOCH: 33], \tTrain Loss: 0.4590\n",
      "[EPOCH: 34], \tTrain Loss: 0.4587\n",
      "[EPOCH: 35], \tTrain Loss: 0.4566\n",
      "[EPOCH: 36], \tTrain Loss: 0.4559\n",
      "[EPOCH: 37], \tTrain Loss: 0.4531\n",
      "[EPOCH: 38], \tTrain Loss: 0.4505\n",
      "[EPOCH: 39], \tTrain Loss: 0.4493\n",
      "[EPOCH: 40], \tTrain Loss: 0.4478\n",
      "[EPOCH: 41], \tTrain Loss: 0.4470\n",
      "[EPOCH: 42], \tTrain Loss: 0.4462\n",
      "[EPOCH: 43], \tTrain Loss: 0.4436\n",
      "[EPOCH: 44], \tTrain Loss: 0.4412\n",
      "[EPOCH: 45], \tTrain Loss: 0.4393\n",
      "[EPOCH: 46], \tTrain Loss: 0.4384\n",
      "[EPOCH: 47], \tTrain Loss: 0.4371\n",
      "[EPOCH: 48], \tTrain Loss: 0.4355\n",
      "[EPOCH: 49], \tTrain Loss: 0.4361\n",
      "[EPOCH: 50], \tTrain Loss: 0.4330\n",
      "[EPOCH: 51], \tTrain Loss: 0.4311\n",
      "[EPOCH: 52], \tTrain Loss: 0.4306\n",
      "[EPOCH: 53], \tTrain Loss: 0.4304\n",
      "[EPOCH: 54], \tTrain Loss: 0.4288\n",
      "[EPOCH: 55], \tTrain Loss: 0.4273\n",
      "[EPOCH: 56], \tTrain Loss: 0.4258\n",
      "[EPOCH: 57], \tTrain Loss: 0.4253\n",
      "[EPOCH: 58], \tTrain Loss: 0.4241\n",
      "[EPOCH: 59], \tTrain Loss: 0.4234\n",
      "[EPOCH: 60], \tTrain Loss: 0.4222\n",
      "[EPOCH: 61], \tTrain Loss: 0.4228\n",
      "[EPOCH: 62], \tTrain Loss: 0.4214\n",
      "[EPOCH: 63], \tTrain Loss: 0.4213\n",
      "[EPOCH: 64], \tTrain Loss: 0.4204\n",
      "[EPOCH: 65], \tTrain Loss: 0.4187\n",
      "[EPOCH: 66], \tTrain Loss: 0.4175\n",
      "[EPOCH: 67], \tTrain Loss: 0.4179\n",
      "[EPOCH: 68], \tTrain Loss: 0.4165\n",
      "[EPOCH: 69], \tTrain Loss: 0.4149\n",
      "[EPOCH: 70], \tTrain Loss: 0.4139\n",
      "[EPOCH: 71], \tTrain Loss: 0.4142\n",
      "[EPOCH: 72], \tTrain Loss: 0.4138\n",
      "[EPOCH: 73], \tTrain Loss: 0.4137\n",
      "[EPOCH: 74], \tTrain Loss: 0.4132\n",
      "[EPOCH: 75], \tTrain Loss: 0.4130\n",
      "[EPOCH: 76], \tTrain Loss: 0.4107\n",
      "[EPOCH: 77], \tTrain Loss: 0.4100\n",
      "[EPOCH: 78], \tTrain Loss: 0.4097\n",
      "[EPOCH: 79], \tTrain Loss: 0.4103\n",
      "[EPOCH: 80], \tTrain Loss: 0.4091\n",
      "[EPOCH: 81], \tTrain Loss: 0.4091\n",
      "[EPOCH: 82], \tTrain Loss: 0.4071\n",
      "[EPOCH: 83], \tTrain Loss: 0.4085\n",
      "[EPOCH: 84], \tTrain Loss: 0.4076\n",
      "[EPOCH: 85], \tTrain Loss: 0.4058\n",
      "[EPOCH: 86], \tTrain Loss: 0.4049\n",
      "[EPOCH: 87], \tTrain Loss: 0.4059\n",
      "[EPOCH: 88], \tTrain Loss: 0.4052\n",
      "[EPOCH: 89], \tTrain Loss: 0.4039\n",
      "[EPOCH: 90], \tTrain Loss: 0.4048\n",
      "[EPOCH: 91], \tTrain Loss: 0.4049\n",
      "[EPOCH: 92], \tTrain Loss: 0.4045\n",
      "[EPOCH: 93], \tTrain Loss: 0.4034\n",
      "[EPOCH: 94], \tTrain Loss: 0.4027\n",
      "[EPOCH: 95], \tTrain Loss: 0.4016\n",
      "[EPOCH: 96], \tTrain Loss: 0.4023\n",
      "[EPOCH: 97], \tTrain Loss: 0.4019\n",
      "[EPOCH: 98], \tTrain Loss: 0.4017\n",
      "[EPOCH: 99], \tTrain Loss: 0.3995\n",
      "[EPOCH: 100], \tTrain Loss: 0.4003\n",
      "[EPOCH: 101], \tTrain Loss: 0.4002\n",
      "[EPOCH: 102], \tTrain Loss: 0.4003\n",
      "[EPOCH: 103], \tTrain Loss: 0.4000\n",
      "[EPOCH: 104], \tTrain Loss: 0.3995\n",
      "[EPOCH: 105], \tTrain Loss: 0.3985\n",
      "[EPOCH: 106], \tTrain Loss: 0.3981\n",
      "[EPOCH: 107], \tTrain Loss: 0.3988\n",
      "[EPOCH: 108], \tTrain Loss: 0.3985\n",
      "[EPOCH: 109], \tTrain Loss: 0.3975\n",
      "[EPOCH: 110], \tTrain Loss: 0.3976\n",
      "[EPOCH: 111], \tTrain Loss: 0.3982\n",
      "[EPOCH: 112], \tTrain Loss: 0.3972\n",
      "[EPOCH: 113], \tTrain Loss: 0.3958\n",
      "[EPOCH: 114], \tTrain Loss: 0.3952\n",
      "[EPOCH: 115], \tTrain Loss: 0.3971\n",
      "[EPOCH: 116], \tTrain Loss: 0.3966\n",
      "[EPOCH: 117], \tTrain Loss: 0.3951\n",
      "[EPOCH: 118], \tTrain Loss: 0.3953\n",
      "[EPOCH: 119], \tTrain Loss: 0.3944\n",
      "[EPOCH: 120], \tTrain Loss: 0.3945\n",
      "[EPOCH: 121], \tTrain Loss: 0.3944\n",
      "[EPOCH: 122], \tTrain Loss: 0.3940\n",
      "[EPOCH: 123], \tTrain Loss: 0.3947\n",
      "[EPOCH: 124], \tTrain Loss: 0.3936\n",
      "[EPOCH: 125], \tTrain Loss: 0.3940\n",
      "[EPOCH: 126], \tTrain Loss: 0.3935\n",
      "[EPOCH: 127], \tTrain Loss: 0.3935\n",
      "[EPOCH: 128], \tTrain Loss: 0.3924\n",
      "[EPOCH: 129], \tTrain Loss: 0.3922\n",
      "[EPOCH: 130], \tTrain Loss: 0.3907\n",
      "[EPOCH: 131], \tTrain Loss: 0.3916\n",
      "[EPOCH: 132], \tTrain Loss: 0.3918\n",
      "[EPOCH: 133], \tTrain Loss: 0.3917\n",
      "[EPOCH: 134], \tTrain Loss: 0.3902\n",
      "[EPOCH: 135], \tTrain Loss: 0.3910\n",
      "[EPOCH: 136], \tTrain Loss: 0.3912\n",
      "[EPOCH: 137], \tTrain Loss: 0.3915\n",
      "[EPOCH: 138], \tTrain Loss: 0.3898\n",
      "[EPOCH: 139], \tTrain Loss: 0.3898\n",
      "[EPOCH: 140], \tTrain Loss: 0.3899\n",
      "[EPOCH: 141], \tTrain Loss: 0.3892\n",
      "[EPOCH: 142], \tTrain Loss: 0.3892\n",
      "[EPOCH: 143], \tTrain Loss: 0.3886\n",
      "[EPOCH: 144], \tTrain Loss: 0.3893\n",
      "[EPOCH: 145], \tTrain Loss: 0.3897\n",
      "[EPOCH: 146], \tTrain Loss: 0.3879\n",
      "[EPOCH: 147], \tTrain Loss: 0.3883\n",
      "[EPOCH: 148], \tTrain Loss: 0.3875\n",
      "[EPOCH: 149], \tTrain Loss: 0.3886\n",
      "[EPOCH: 150], \tTrain Loss: 0.3881\n",
      "[EPOCH: 151], \tTrain Loss: 0.3867\n",
      "[EPOCH: 152], \tTrain Loss: 0.3873\n",
      "[EPOCH: 153], \tTrain Loss: 0.3882\n",
      "[EPOCH: 154], \tTrain Loss: 0.3879\n",
      "[EPOCH: 155], \tTrain Loss: 0.3874\n",
      "[EPOCH: 156], \tTrain Loss: 0.3856\n",
      "[EPOCH: 157], \tTrain Loss: 0.3858\n",
      "[EPOCH: 158], \tTrain Loss: 0.3867\n",
      "[EPOCH: 159], \tTrain Loss: 0.3859\n",
      "[EPOCH: 160], \tTrain Loss: 0.3852\n",
      "[EPOCH: 161], \tTrain Loss: 0.3845\n",
      "[EPOCH: 162], \tTrain Loss: 0.3862\n",
      "[EPOCH: 163], \tTrain Loss: 0.3845\n",
      "[EPOCH: 164], \tTrain Loss: 0.3855\n",
      "[EPOCH: 165], \tTrain Loss: 0.3843\n",
      "[EPOCH: 166], \tTrain Loss: 0.3851\n",
      "[EPOCH: 167], \tTrain Loss: 0.3844\n",
      "[EPOCH: 168], \tTrain Loss: 0.3835\n",
      "[EPOCH: 169], \tTrain Loss: 0.3844\n",
      "[EPOCH: 170], \tTrain Loss: 0.3844\n",
      "[EPOCH: 171], \tTrain Loss: 0.3832\n",
      "[EPOCH: 172], \tTrain Loss: 0.3825\n",
      "[EPOCH: 173], \tTrain Loss: 0.3827\n",
      "[EPOCH: 174], \tTrain Loss: 0.3837\n",
      "[EPOCH: 175], \tTrain Loss: 0.3827\n",
      "[EPOCH: 176], \tTrain Loss: 0.3843\n",
      "[EPOCH: 177], \tTrain Loss: 0.3833\n",
      "[EPOCH: 178], \tTrain Loss: 0.3827\n",
      "[EPOCH: 179], \tTrain Loss: 0.3834\n",
      "[EPOCH: 180], \tTrain Loss: 0.3833\n",
      "[EPOCH: 181], \tTrain Loss: 0.3826\n",
      "[EPOCH: 182], \tTrain Loss: 0.3818\n",
      "[EPOCH: 183], \tTrain Loss: 0.3811\n",
      "[EPOCH: 184], \tTrain Loss: 0.3825\n",
      "[EPOCH: 185], \tTrain Loss: 0.3826\n",
      "[EPOCH: 186], \tTrain Loss: 0.3803\n",
      "[EPOCH: 187], \tTrain Loss: 0.3822\n",
      "[EPOCH: 188], \tTrain Loss: 0.3808\n",
      "[EPOCH: 189], \tTrain Loss: 0.3805\n",
      "[EPOCH: 190], \tTrain Loss: 0.3815\n",
      "[EPOCH: 191], \tTrain Loss: 0.3806\n",
      "[EPOCH: 192], \tTrain Loss: 0.3805\n",
      "[EPOCH: 193], \tTrain Loss: 0.3801\n",
      "[EPOCH: 194], \tTrain Loss: 0.3802\n",
      "[EPOCH: 195], \tTrain Loss: 0.3789\n",
      "[EPOCH: 196], \tTrain Loss: 0.3809\n",
      "[EPOCH: 197], \tTrain Loss: 0.3792\n",
      "[EPOCH: 198], \tTrain Loss: 0.3785\n",
      "[EPOCH: 199], \tTrain Loss: 0.3791\n",
      "[EPOCH: 200], \tTrain Loss: 0.3799\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "for epoch in range(0, EPOCHS + 1):\n",
    "  train_loss=train(model, train_loader,optimizer,criterion)\n",
    "  #val_loss = evaluate(model, test_loader,criterion)\n",
    "  print(f\"[EPOCH: {epoch}], \\tTrain Loss: {train_loss:.4f}\")\n",
    "  result = {\n",
    "    'EPOCH': epoch,\n",
    "    'Train Loss': train_loss,\n",
    "    #'Val Loss': val_loss,\n",
    "    #'Val Accuracy': val_accuracy\n",
    "    }\n",
    "  \n",
    "  result_list.append(result)\n",
    "result_df = pd.DataFrame(result_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_df['Train Loss'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prob :  [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.0527741  0.         0.0466271  0.         0.         0.08683446\n",
      " 0.1081726  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.1223163  0.29278728 0.38952166\n",
      " 0.45468202 0.4487188  0.22605014 0.2669046  0.05412975 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.15892598 0.3791368  0.6297027  0.56459856 0.53289425 0.49082416\n",
      " 0.2369115  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12019539 0.3842091  0.6056309\n",
      " 0.6154528  0.5749588  0.5882163  0.44013312 0.18445277 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.15496987 0.49106365 0.54955447 0.44652873 0.4441294\n",
      " 0.3793419  0.11282656 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.27746636\n",
      " 0.28783256 0.3108652  0.2579842  0.16510662 0.04057217 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.10935226\n",
      " 0.04531872 0.16161036 0.         0.06859648 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.34273818 0.31501603 0.3409605\n",
      " 0.35756224 0.25918847 0.05076948 0.         0.         0.\n",
      " 0.0324401  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.10977414\n",
      " 0.44244733 0.512507   0.49859464 0.5574145  0.4996471  0.35528994\n",
      " 0.2701326  0.         0.         0.         0.22137192 0.19668406\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.51858383 0.70210713\n",
      " 0.7264045  0.67569655 0.6455079  0.54347587 0.35075498 0.\n",
      " 0.         0.         0.         0.09452507 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.2670095  0.48765004 0.57847077 0.7014917  0.6214296\n",
      " 0.5810873  0.4837767  0.2238101  0.         0.         0.\n",
      " 0.10841298 0.10996297 0.         0.00350347 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.05897543 0.         0.08971721\n",
      " 0.3208922  0.48770285 0.5833309  0.53291297 0.51118857 0.3630249\n",
      " 0.13270348 0.04254383 0.         0.1655317  0.08247301 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.03356498 0.30017585\n",
      " 0.3871173  0.4163741  0.43324268 0.30694824 0.251316   0.\n",
      " 0.00724325 0.0067848  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.06842774 0.09603941 0.03312206 0.18434587 0.01190197\n",
      " 0.03807932 0.         0.         0.02345914 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00446653 0.         0.02496356 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03931522 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "raw, prob = model.get_gates()\n",
    "#print('raw : ', raw)\n",
    "print(\" prob : \", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test=train_set[100]\n",
    "lst = list(test)\n",
    "test = lst[0][0]\n",
    "test_prob=prob.reshape(28,28)\n",
    "final=test*test_prob\n",
    "\n",
    "fig =  plt.figure(figsize=(10,20))\n",
    "rows = 1 \n",
    "cols = 2\n",
    "\n",
    "ax1 = fig.add_subplot(rows,cols,1)\n",
    "ax1.imshow(test)\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2= fig.add_subplot(rows,cols,2)\n",
    "ax2.imshow(final)\n",
    "ax2.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tobigs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
